<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models">
  <meta name="keywords" content="HiF-VLA, HiF_VLA, HiFVLA, hifvla, hif-vla, hif_vla">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <title>HiF-VLA</title>


  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href='data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 256"><text x="50%" y="50%" font-size="200" dominant-baseline="middle" text-anchor="middle">ü§ñ</text></svg>'> -->
  <link rel="icon" type="image/png" href="./static/images/icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body " style="background-color: hsl(0, 3%, 87%);">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">HiF-VLA: <strong>H</strong>indsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hifvla.github.io/">Minghui Lin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://dingpx.github.io/">Pengxiang Ding</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hifvla.github.io/">Shu Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-KANvNMAAAAJ&hl=zh-CN">Zifeng Zhuang</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yliu-cs.github.io/">Yang Liu</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=UzxoKnkAAAAJ&hl=zh-CN">Xinyang Tong</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://songwxuan.github.io/">Wenxuan Song</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=3_DtxJ8AAAAJ&hl=zh-CN">Shangle Lyu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://kyonhuang.top/">Siteng Huang</a><sup>2,‚úâ</sup>,
            </span>
            <span class="author-block">
              <a href="https://milab.westlake.edu.cn/">Donglin Wang</a><sup>1,5,‚úâ</sup>,
            </span>
          </div>
            

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Westlake University</span>&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Zhejiang University</span>&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>HKUST(GZ)</span>&nbsp;&nbsp;
            <span class="author-block"><sup>4</sup>Nanjing University</span>
            <span class="author-block"><sup>5</sup>Westlake Robotics</span>
            <br>
            <!-- <span class="author-block"><sup>*</sup>Equal Contribution</span>&nbsp;&nbsp; -->
            <!-- <span class="author-block"><sup>‚Ä°</sup>Project Lead</span>&nbsp;&nbsp; -->
            <span class="author-block"><sup>‚úâ</sup>Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
               <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.09928"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> 
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.09928"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/OpenHelix-Team/HiF-VLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/minnielin/hifvla-libero-long"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Models</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/minnielin/libero_trajid_rlds"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Dataset</span>
                </a>
              </span>              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section ">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. 
Building on this idea, we propose <strong>HiF-VLA</strong> (<strong>Hindsight, Insight, and Foresight</strong> for VLAs)</strong>, a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a "<strong>think-while-acting</strong>" paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
    
  <section class="section-container">
  
    <h2 class="title is-3">Motivation & Insight</h2>
    <div class="columns is-multiline is-centered">

      <figure class="image is-is-16by9">
        <img src="./static/images/motivation.png" alt="Models." style="width:100%; height:auto;">
      </figure>

      <!-- <p>
        Our core philosophy is to enhance the model's spatial awareness using an external guide. To provide our model with a rich understanding of 3D space, we leverage a powerful, pre-trained 3D foundation model VGGT. For any given set of multi-view images, this foundation model generates detailed, pixel-level spatial representations‚Äîeffectively creating a 3D-aware feature map from 2D inputs. These representations are further enriched with positional embeddings to maintain critical structural information.
      </p> -->
    </div>
    <div class="content-block">
      <h3 class="subsection-title">‚ö†Ô∏è The Challenge: Temporal Myopia</h3>
      <p>
        Most existing VLAs implicitly assume a Markov property, predicting actions solely from current observations. This leads to <strong>temporal myopia</strong>. Common solutions like <span class="highlight">frame stacking</span> are computationally expensive and introduce massive pixel-level redundancy, obscuring key dynamics.
      </p>
    </div>
  
    <div class="content-block insight-block">
      <h3 class="subsection-title">üí° Our Insight</h3>
      <p class="insight-text">
        We argue that <strong>motion</strong>‚Äîrather than raw pixels‚Äîis the most precise and compact proxy for history. It captures critical dynamic interactions while explicitly filtering out static visual redundancy.
      </p>
      <p class="insight-text">
        Furthermore, robust decision-making demands <strong>bidirectional temporal reasoning</strong>. Motion acts as the natural bridge unifying the past (Hindsight) and the future (Foresight).
      </p>
    </div>
  
    <div class="content-block">
      <h3 class="subsection-title">üöÄ The Solution and Perfomance</h3>
    <li>
      To bridge this gap, we introduce <strong>HiF-VLA</strong>, a unified framework utilizing motion-centric bidirectional spatio-temporal reasoning. By concurrently predicting motion and action while maintaining temporal consistency, 
      it enables a robust <strong><em>"Think-While-Acting"</em></strong> paradigm.
      </li>
     <li>
      HiF-VLA demonstrates superior efficiency and scalability. It reduces inference latency by 58.3% compared to frame-stacking methods while achieving state-of-the-art performance on LIBERO-Long (96.4%), Calvin, and complex real-world manipulation tasks.
      </li>
    </div>
  </section>
  
  <style>
    /* Âü∫Á°ÄÂÆπÂô®ËÆæÁΩÆÔºåÁ°Æ‰øùÊñáÂ≠ó‰∏ç‰ºöÂ§™ÂÆΩÔºå‰æø‰∫éÈòÖËØª */
    .section-container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem 1rem;
      font-family: "Noto Sans", "Inter", sans-serif; /* Â≠¶ÊúØÂ∏∏Áî®Â≠ó‰Ωì */
      color: #333;
      line-height: 1.6;
    }
  
    /* Ê†áÈ¢òÊ†∑Âºè */
    .section-title {
      font-size: 2rem;
      font-weight: 700;
      text-align: center;
      margin-bottom: 2.5rem;
      color: #111;
    }
  
    .subsection-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 0.8rem;
      color: #000;
    }
  
    /* ÂÜÖÂÆπÂùóÈó¥Ë∑ù */
    .content-block {
      margin-bottom: 2.5rem;
    }
  
    /* Âè™ÊúâÁÆÄÂçïÁöÑ‰∏ãÂàíÁ∫øÈ´ò‰∫ÆÔºåÊ≤°ÊúâÈ¢úËâ≤Âùó */
    .highlight {
      text-decoration: underline;
      text-decoration-color: #aaa;
      text-decoration-thickness: 2px;
    }
  
    /* Insight ÈÉ®ÂàÜÔºöÂ∑¶‰æßÁ´ñÁ∫øÂº∫Ë∞ÉÔºåÊûÅÁÆÄÈ£éÊ†º */
    .insight-block {
      border-left: 4px solid #444; /* Ê∑±ÁÅ∞Ëâ≤Á´ñÁ∫ø */
      padding-left: 1.5rem;
      margin-left: 0.5rem;
    }
  
    .insight-text {
      font-size: 1rem; /* Á®çÂæÆÂ§ß‰∏ÄÁÇπÁöÑÂ≠óÂè∑ */
      font-style: italic; /* Êñú‰ΩìÂº∫Ë∞É */
      color: #444;
      margin-bottom: 0.8rem;
    }
    
    .insight-text strong {
      color: #000;
      font-style: normal;
    }
  
    /* ÂàóË°®Ê†∑Âºè */
    .method-list {
      list-style-type: none; /* ÂéªÊéâÈªòËÆ§ÂúÜÁÇπ */
      padding-left: 0;
    }
  
    .method-list li {
      margin-bottom: 1rem;
      position: relative;
      padding-left: 1.5rem;
    }
  
    /* Ëá™ÂÆö‰πâÁÆÄÂçïÁöÑÂúÜÁÇπ */
    .method-list li::before {
      content: "‚Ä¢";
      position: absolute;
      left: 0;
      color: #666;
      font-weight: bold;
    }
  
    .method-name {
      font-weight: 700;
      color: #000;
    }
  </style>

<section class="section ">
<!-- <div class="container is-max-desktop"> -->
<div class="columns is-centered has-text-centered"> 
  <!-- column ÂÆπÂô®ÊòØÂøÖÈ°ªÁöÑÔºåÂÆÉÊéßÂà∂ÂÜÖÂÆπÁöÑÂÆΩÂ∫¶ÂíåÂØπÈΩê -->
  <div class="column is-four-fifths">
    <!-- Ê†áÈ¢ò -->
    <h2 class="title is-3">Method</h2>

    <!-- ÂåÖÂê´ÊâÄÊúâÊÆµËêΩÁöÑ content ÂÆπÂô® -->
    <div class="content has-text-justified">
      <figure class="image is-is-16by9">
        <img src="./static/images/hifvla.png" alt="Models." style="width:100%; height:auto;">
      </figure>
      <p>
        <strong>The HiF-VLA Framework.</strong> Our approach unifies perception, reasoning, and action through three key stages: 
      (a) Hindsight Prior Acquisition, (b) Foresight Reasoning with Insight, and (c) Hindsight-Modulated Joint Expert.
      </p>
      <li>
        <strong>Hindsight Prior Acquisition</strong> : Instead of stacking raw image frames, we encode historical context into structured, low-dimensional <strong>Motion Vectors (MVs)</strong>. This representation efficiently serves as the "Hindsight," preserving essential dynamics while discarding pixel-level redundancy.
      </li>
      
      <li>
        <strong>Foresight Reasoning with Insight</strong>: Leveraging the reasoning capabilities of VLMs, the model interprets task instructions and the current observation ("Insight"). It anticipates plausible <strong>Foresight Motions</strong> and generates latent action tokens. 
      </li>
      <li>
        <strong>Hindsight-Modulated Joint Expert</strong>: We introduce a <strong>Joint Expert</strong> where the <em>Hindsight</em> (past motion) acts as a constraint to modulate the <em>Foresight</em> and <em>Action</em> streams. This modulation ensures that generated actions are causally consistent and temporally coherent.
      </li>
    </div>

  </div> <!-- ËøôÊòØ is-four-fifths column ÁöÑÁªìÊùüÊ†áÁ≠æ -->

</div> 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
    <br> 

<h3 class="title is-4">Evaluation on LIBERO Benchmark</h3>
<div class="content has-text-justified">
  <figure class="image is-is-16by9">
      <img src="./static/images/LIBERO.png" style="width: 100%; height: auto; display: block; margin: auto;" alt="Comparisons with state-of-the-art methods on LIBERO benchmark.">
      <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
        Comparisons with state-of-the-art methods on the LIBERO benchmark.
      </figcaption>
    </figure>
  <p>
    HiF-VLA establishes a new state-of-the-art on the LIBERO benchmark, demonstrating significant gains especially in complex, long-horizon manipulation tasks.
  </p>
</div>

<h3 class="title is-4">Evaluation on CALVIN-ABC Benchmark</h3>
<div class="content has-text-justified">
  <figure class="image is-is-21by9">
      <img src="./static/images/calvin_abc.png" style="width: 80%; height: auto; display: block; margin: auto;" alt="Comparisons with state-of-the-art methods on CALVIN-ABC benchmark.">
      <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
        Comparisons with state-of-the-art methods on CALVIN-ABC benchmark.
      </figcaption>
    </figure>
  <p>
    On the Calvin benchmark, our method exhibits superior performance, outperforming existing approaches in average sequence length across both third-view and multi-view settings.
  </p>
</div>

<h3 class="title is-4">Evaluation on Real World</h3>
<div class="content has-text-justified">
  <b>
    We conduct experiments on three real-world long-horizon tasks, shown in the videos below at the original speed of robot actions.
  </b>
</div>

<!-- demo videos -->
<div class="content has-text-justified" style="
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 0em;
  flex-wrap: wrap;
">
  <figure class="image" style="flex: 1; text-align: center; margin: 0;">
    <video class="video" controls autoplay muted loop playsinline
      style="width:80%; height:auto; vertical-align: top; border-radius: 12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);">
      <source src="./static/videos/place_blocks.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
      Place blocks on the plates.
    </figcaption>
  </figure>
  <figure class="image" style="flex: 1; text-align: center; margin: 0;">
    <video class="video" controls autoplay muted loop playsinline
      style="width:80%; height:auto; vertical-align: top; border-radius: 12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);">
      <source src="./static/videos/cover_and_stack.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
      Cover block and stack bowls.
    </figcaption>
  </figure>
</div>

<div class="content has-text-justified" style="
  display: flex;
  justify-content: center;
  align-items: flex-start;
  gap: 1em;
  flex-wrap: wrap;
">
  <figure class="image" style="flex: 1; text-align: center; margin: 0;">
    <video class="video" controls autoplay muted loop playsinline
      style="width:42%; height:auto; vertical-align: top; border-radius: 12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);">
      <source src="./static/videos/press_buttons.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
      Press buttons in order.
    </figcaption>
  </figure>
  <!-- <figure class="image" style="flex: 1; text-align: center; margin: 0;">
    <video class="video" controls autoplay muted loop playsinline
      style="width:80%; height:auto; vertical-align: top; border-radius: 12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);">
      <source src="./static/videos/stack_floor.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption class="has-text-centered is-size-6" style="margin-top: 0.5em;">
      Place Green Block.
    </figcaption>
  </figure> -->
</div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{lin2025hifvlahindsightinsightforesight,
      title={HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models}, 
      author={Minghui Lin and Pengxiang Ding and Shu Wang and Zifeng Zhuang and Yang Liu and Xinyang Tong and Wenxuan Song and Shangke Lyu and Siteng Huang and Donglin Wang},
      year={2025},
      eprint={2512.09928},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2512.09928}, 
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
